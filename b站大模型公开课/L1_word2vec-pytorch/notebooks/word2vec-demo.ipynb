{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPkRFyBijFStqrKY4s8vBCA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### word2vec demo\n"],"metadata":{"id":"W0h-7IeidjLA"}},{"cell_type":"markdown","source":["#### Wrod2vec简要介绍\n","Word2Vec（Word to Vector）是一种用于将单词表示为向量的技术，它是由Tomas Mikolov等人在2013年提出的。这种方法通过将单词映射到高维空间中的向量，使得具有相似含义的单词在这个空间中的表示也是相近的。\n","\n","Word2Vec有两种主要的模型结构：Skip-gram和Continuous Bag of Words (CBOW)。\n","\n","Word2Vec的应用包括：\n","\n","*   词向量表示： 将单词表示为向量，使得可以在向量空间中度量词汇之间的语义相似性。\n","*   文本相似性分析： 基于词向量，可以度量文本之间的相似性，用于文本分类、聚类等任务。\n","*   信息检索： 使用词向量来改进搜索引擎的检索效果。\n","*   推荐系统： 在协同过滤等算法中使用词向量来推荐相关的内容。\n","\n","work2vec的输出并不是我们真正需要的，真正需要的是embedding层的权重矩阵（lookup table），通过将任意一个one-hot编码的词左乘这个矩阵就得到了低维绸密的词向量,又因为输入为one-hot编码，所以权重矩阵每一行的就是最终的词向量\n"],"metadata":{"id":"OoCmF-NGRJw4"}},{"cell_type":"code","source":["# 可以挂载google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnDO8Ep4n-6D","executionInfo":{"status":"ok","timestamp":1704103622429,"user_tz":-480,"elapsed":29062,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"7ee9b951-9eed-49d9-fac3-c5ff489c7b89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#### 拉取代码\n","测试的代码来自：[word2vec-pytorch](https://OlgaChernytska/word2vec-pytorch)\n","\n","在colan中使用linux命令前面要加%，bash命令加!，pro会员也可以直接使用terminal\n","\n","从git上拉取代码, 并切换到工作目录"],"metadata":{"id":"1tTdux_RtpUe"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive\n","%mkdir nlp-project\n","%cd nlp-project\n","\n","!git clone https://github.com/OlgaChernytska/word2vec-pytorch.git\n","\n","%cd word2vec-pytorch/\n","\n","%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"id":"IERv3kijtEuK","executionInfo":{"status":"ok","timestamp":1704103672287,"user_tz":-480,"elapsed":3645,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"d9ca0bbd-7cb4-4b04-c5fb-6cabe7b365db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","/content/drive/MyDrive/nlp-project\n","Cloning into 'word2vec-pytorch'...\n","remote: Enumerating objects: 144, done.\u001b[K\n","remote: Counting objects: 100% (39/39), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 144 (delta 30), reused 25 (delta 25), pack-reused 105\u001b[K\n","Receiving objects: 100% (144/144), 18.45 MiB | 17.46 MiB/s, done.\n","Resolving deltas: 100% (60/60), done.\n","/content/drive/MyDrive/nlp-project/word2vec-pytorch\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/nlp-project/word2vec-pytorch'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#### 数据集\n","demo使用的数据集为wikitext-2和wikitext-103，它们是从Wikipedia的优秀和精选文章中提取的超过1亿条词汇的文本数据集。[homepage](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/#examples)\n","\n","example：\n","\n","= = = Hurricane Nine = = =\n","\n"," On September 14 , a tropical cyclone formed off the coast of Central America . This tropical storm tracked northwestward and intensified into a hurricane . The sea @-@ level pressure dropped to 975 mbar ( 28 @.@ 8 inHg ) or lower . The hurricane recurved gradually to the northeast and weakened over cool seas . On September 25 , this tropical storm made landfall near Long Beach , California , and dissipated inland .\n"," The tropical storm caught Southern Californians unprepared . It brought heavy rain and flooding to the area , which killed 45 people . At sea , 48 were killed . The storm caused heavy property damage amounting to $ 2 million ( 1939 USD ) in total , mostly to crops and coastal infrastructure ."],"metadata":{"id":"ACDMIdpgUqjD"}},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RAvwGKmWLTS","executionInfo":{"status":"ok","timestamp":1704103677032,"user_tz":-480,"elapsed":806,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"e6bbbc07-895d-4a94-913a-ef3229bca06b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config.yaml  \u001b[0m\u001b[01;34mdocs\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  README.md  requirements.txt  train.py  \u001b[01;34mutils\u001b[0m/  \u001b[01;34mweights\u001b[0m/\n"]}]},{"cell_type":"code","source":["# 使用gpu，cpu训练太慢。。\n","import torch\n","device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"We3UqjrOcI4_","executionInfo":{"status":"ok","timestamp":1704103680275,"user_tz":-480,"elapsed":609,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"7658ddae-351d-4d43-a170-0696c5cc0024"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["#### 训练并保存相关文件\n","删除目标路径\n","\n","可以修改config.yaml文件来切换模型、更改学习率或epoch等参数\n","\n","运行程序"],"metadata":{"id":"IFDZsk00PIpj"}},{"cell_type":"code","source":["!rm -rf weights/cbow_WikiText2/\n","!pip install portalocker\n","!python train.py --config config.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IjBBDoxl9fJn","executionInfo":{"status":"ok","timestamp":1704103735243,"user_tz":-480,"elapsed":51850,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"1a765d8c-4a41-4c92-ecb3-1c53c6ab662f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n","Vocabulary size: 4099\n","Adjusting learning rate of group 0 to 2.5000e-02.\n","Epoch: 1/5, Train Loss=5.29369, Val Loss=5.02354\n","Adjusting learning rate of group 0 to 2.0000e-02.\n","Epoch: 2/5, Train Loss=4.96373, Val Loss=4.90678\n","Adjusting learning rate of group 0 to 1.5000e-02.\n","Epoch: 3/5, Train Loss=4.84497, Val Loss=4.83733\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 4/5, Train Loss=4.75150, Val Loss=4.76742\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 5/5, Train Loss=4.64980, Val Loss=4.68193\n","Adjusting learning rate of group 0 to 0.0000e+00.\n","Training finished.\n","Model artifacts saved to folder: weights/cbow_WikiText2\n"]}]},{"cell_type":"code","source":["%cd weights/cbow_WikiText2/\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kR_d7gadaIP","executionInfo":{"status":"ok","timestamp":1704103743219,"user_tz":-480,"elapsed":4,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"53c82983-0cd2-4361-b1ed-f62c43c55aac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nlp-project/word2vec-pytorch/weights/cbow_WikiText2\n","config.yaml  loss.json  model.pt  vocab.pt\n"]}]},{"cell_type":"markdown","source":["通过挂载谷歌硬盘，将代码、模型文件及词汇表文件保存到硬盘中，这样下次可以直接使用，而不是每次重新训练模型\n"],"metadata":{"id":"BXZ3KsMadiWB"}},{"cell_type":"code","source":["# 可以挂载google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","\n","%cd /content/drive/MyDrive/nlp-project/word2vec-pytorch/weights/cbow_WikiText2\n","%ll"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTrWBldlggev","executionInfo":{"status":"ok","timestamp":1704103989096,"user_tz":-480,"elapsed":27300,"user":{"displayName":"tianyu jia","userId":"10449023397212915774"}},"outputId":"93ea2ac7-d453-422f-9b34-fd226d1e5c2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/nlp-project/word2vec-pytorch/weights/cbow_WikiText2\n","total 9693\n","-rw------- 1 root     248 Jan  1 10:08 config.yaml\n","-rw------- 1 root     214 Jan  1 10:08 loss.json\n","-rw------- 1 root 9856902 Jan  1 10:08 model.pt\n","-rw------- 1 root   67480 Jan  1 10:08 vocab.pt\n"]}]},{"cell_type":"markdown","source":["#### 一些说明\n","\n","model.pt为troch保存的模型文件\n","\n","vocab.pt为词表对象文件(torchtext.vocab.Vocab: A `Vocab` object)，它用于存储单词到index的映射以及相关的统计信息\n","\n","example：\n","```\n",">>> from torchtext.vocab import vocab\n",">>> from collections import Counter, OrderedDict\n",">>> counter = Counter([\"a\", \"a\", \"b\", \"b\", \"b\"])\n",">>> sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",">>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",">>> v1 = vocab(ordered_dict)\n",">>> print(v1['a']) #prints 1\n",">>> print(v1['out of vocab']) #raise RuntimeError since default index is not set\n",">>> tokens = ['e', 'd', 'c', 'b', 'a']\n",">>> #adding <unk> token and default index\n",">>> unk_token = '<unk>'\n",">>> default_index = -1\n",">>> v2 = vocab(OrderedDict([(token, 1) for token in tokens]), specials=[unk_token])\n",">>> v2.set_default_index(default_index)\n",">>> print(v2['<unk>']) #prints 0\n",">>> print(v2['out of vocab']) #prints -1\n",">>> #make default index same as index of unk_token\n",">>> v2.set_default_index(v2[unk_token])\n",">>> v2['out of vocab'] is v2[unk_token] #prints True\n","```"],"metadata":{"id":"e4NRFpwcg9Tc"}}]}